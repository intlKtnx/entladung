{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d550625c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "from torch.utils import data\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8694bcbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrapping the dataset with load function\n",
    "class CustomDataset(data.Dataset):\n",
    "    def __init__(self, file_path, transform=None):\n",
    "        super().__init__()\n",
    "        self.train_data_cache = []\n",
    "        self.test_data_cache = []\n",
    "        self.manual_data_cache = []\n",
    "        self.transform = transform\n",
    "        self.label_count = [0, 0, 0]\n",
    "        # Search for all h5 files\n",
    "        p = Path(file_path)\n",
    "        files = p.glob('*.h5')\n",
    "        logging.debug(files)\n",
    "        for h5dataset_fp in files:\n",
    "            logging.debug(h5dataset_fp)\n",
    "            with h5py.File(h5dataset_fp.resolve()) as h5_file:\n",
    "                # Walk through all groups, extracting datasets\n",
    "                for gname, group in h5_file.items():\n",
    "                    k = 0\n",
    "                    j = 0\n",
    "                    l = 0\n",
    "                    if gname == 'referenz':\n",
    "                        label = 0\n",
    "                    elif gname == 'spitze':\n",
    "                        label = 1\n",
    "                    elif gname == 'grenzflaeche':\n",
    "                        label = 2\n",
    "\n",
    "                    logging.debug(group.items())\n",
    "                    for dname, ds in tqdm(group.items()):\n",
    "                        if k < 3000:\n",
    "                            for i in np.split(ds, 4):\n",
    "                                self.train_data_cache.append([label, torch.tensor(i).unsqueeze(0).type(torch.float32)])\n",
    "                                k += 1\n",
    "                        if j < 400:\n",
    "                            for i in np.split(ds, 4):\n",
    "                                self.test_data_cache.append([label, torch.tensor(i).unsqueeze(0).type(torch.float32)])\n",
    "                                j += 1\n",
    "                        if l < 100:\n",
    "                            for i in np.split(ds, 4):\n",
    "                                self.manual_data_cache.append([label, torch.tensor(i).unsqueeze(0).type(torch.float32)])\n",
    "                                l += 1\n",
    "\n",
    "                        if k > 3000 and j > 400:\n",
    "                            break\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.data_cache[index]\n",
    "\n",
    "    def get_test_data(self):\n",
    "        return self.test_data_cache\n",
    "\n",
    "    def get_train_data(self):\n",
    "        return self.train_data_cache\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data_cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c43ba60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the network\n",
    "class Network(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(1, 16, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(16, 32, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(32, 64, kernel_size=3, padding=1)\n",
    "        self.conv4 = nn.Conv1d(64, 128, kernel_size=3, padding=1)  # endsize 1536 maxpool 3\n",
    "        self.maxPool1 = nn.MaxPool1d(3)\n",
    "        self.maxPool2 = nn.MaxPool1d(3)\n",
    "        self.maxPool3 = nn.MaxPool1d(3)\n",
    "        self.maxPool4 = nn.MaxPool1d(3)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.relu4 = nn.ReLU()\n",
    "        # self.conv5 = nn.Conv1d(128, 256, kernel_size=3, padding=1) #endsize 1024 maxpool 3\n",
    "        # self.conv6 = nn.Conv1d(256, 512, kernel_size=3, padding=1) # endsize 512 maxpool 3\n",
    "\n",
    "        self.fc1 = nn.Linear(384, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool1d(F.relu(self.conv1(x)), 3)\n",
    "        x = F.max_pool1d(F.relu(self.conv2(x)), 3)\n",
    "        x = F.max_pool1d(F.relu(self.conv3(x)), 3)\n",
    "        x = F.max_pool1d(F.relu(self.conv4(x)), 3)\n",
    "        # x = F.max_pool1d(F.relu(self.conv5(x)),3)\n",
    "        # x = F.max_pool1d(F.relu(self.conv6(x)),3)\n",
    "        #logging.debug(x.shape)\n",
    "        x = torch.flatten(x, 1)\n",
    "        #logging.debug(x.shape)\n",
    "        x = F.softmax(self.fc1(x), dim=1)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95f6ffbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# the train loop\n",
    "def train(dataloader, optimizer, criterion, model):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    j = 0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # get the inputs; data is a list of [labels, inputs]\n",
    "\n",
    "        inputs = data[1]\n",
    "        labels = data[0]\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # log statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 500 == 499:\n",
    "            logging.debug(f\"[{epoch}]Loss: {running_loss / 500} \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8e3fa49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing the accuracy on single 1024 snippets\n",
    "def split_test(dataloader, optimizer, criterion, model):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for labels, inputs in dataloader:\n",
    "            labels, inputs = labels.to(device), inputs.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            correct += (outputs.argmax(1) == labels).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\" Random Teilstück Error: Accuracy: {(100 * correct):>0.1f}%, Avg loss: {test_loss:>8f}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99bcb720",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# looking for cuda device and selecting it if possible\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Using {} device'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3772bbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 4000/4000 [00:00<00:00, 7389.01it/s]\n",
      "100%|████████████████████████████████| 148022/148022 [00:11<00:00, 12863.30it/s]\n",
      "100%|█████████████████████████████████████| 3401/3401 [00:00<00:00, 7779.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# loading the data\n",
    "customData = CustomDataset(\"/home/marcus/Dokumente/entladung/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4127cc14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Network(\n",
      "  (conv1): Conv1d(1, 16, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (conv2): Conv1d(16, 32, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (conv3): Conv1d(32, 64, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (conv4): Conv1d(64, 128, kernel_size=(3,), stride=(1,), padding=(1,))\n",
      "  (maxPool1): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  (maxPool2): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  (maxPool3): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  (maxPool4): MaxPool1d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)\n",
      "  (relu1): ReLU()\n",
      "  (relu2): ReLU()\n",
      "  (relu3): ReLU()\n",
      "  (relu4): ReLU()\n",
      "  (fc1): Linear(in_features=384, out_features=3, bias=True)\n",
      ")\n",
      "33699\n"
     ]
    }
   ],
   "source": [
    "# defining the model and moving it to the correct device\n",
    "model = Network().to(device)\n",
    "print(model)\n",
    "total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "29ed711f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f671b27f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining train and test sets\n",
    "train_data = customData.get_train_data()\n",
    "test_data = customData.get_test_data()\n",
    "train_dataloader = DataLoader(train_data, batch_size=256, shuffle=True, pin_memory=False, num_workers=4)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=True, pin_memory=False, num_workers=4)\n",
    "manual_dataloader = DataLoader(customData.manual_data_cache, batch_size=4, shuffle=False, pin_memory=False, num_workers=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1641ee5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Random Teilstück Error: Accuracy: 88.3%, Avg loss: 0.679858\n",
      " Random Teilstück Error: Accuracy: 88.9%, Avg loss: 0.667332\n",
      " Random Teilstück Error: Accuracy: 90.4%, Avg loss: 0.643285\n",
      " Random Teilstück Error: Accuracy: 91.0%, Avg loss: 0.642901\n",
      " Random Teilstück Error: Accuracy: 90.8%, Avg loss: 0.639120\n",
      " Random Teilstück Error: Accuracy: 91.7%, Avg loss: 0.637110\n",
      " Random Teilstück Error: Accuracy: 91.0%, Avg loss: 0.636783\n",
      " Random Teilstück Error: Accuracy: 91.7%, Avg loss: 0.634563\n",
      " Random Teilstück Error: Accuracy: 91.6%, Avg loss: 0.634240\n",
      " Random Teilstück Error: Accuracy: 92.0%, Avg loss: 0.637142\n",
      " Random Teilstück Error: Accuracy: 91.0%, Avg loss: 0.637228\n",
      " Random Teilstück Error: Accuracy: 91.2%, Avg loss: 0.635636\n",
      " Random Teilstück Error: Accuracy: 90.5%, Avg loss: 0.637802\n",
      " Random Teilstück Error: Accuracy: 91.4%, Avg loss: 0.634818\n",
      " Random Teilstück Error: Accuracy: 90.8%, Avg loss: 0.637352\n",
      " Random Teilstück Error: Accuracy: 92.3%, Avg loss: 0.628618\n",
      " Random Teilstück Error: Accuracy: 92.8%, Avg loss: 0.626196\n",
      " Random Teilstück Error: Accuracy: 92.8%, Avg loss: 0.623981\n",
      " Random Teilstück Error: Accuracy: 93.5%, Avg loss: 0.621095\n",
      " Random Teilstück Error: Accuracy: 93.5%, Avg loss: 0.617812\n",
      " Random Teilstück Error: Accuracy: 93.5%, Avg loss: 0.619321\n",
      " Random Teilstück Error: Accuracy: 94.0%, Avg loss: 0.615760\n",
      " Random Teilstück Error: Accuracy: 94.2%, Avg loss: 0.613755\n",
      " Random Teilstück Error: Accuracy: 94.3%, Avg loss: 0.612267\n",
      " Random Teilstück Error: Accuracy: 94.9%, Avg loss: 0.610336\n",
      " Random Teilstück Error: Accuracy: 92.3%, Avg loss: 0.625003\n",
      " Random Teilstück Error: Accuracy: 94.7%, Avg loss: 0.609089\n",
      " Random Teilstück Error: Accuracy: 94.9%, Avg loss: 0.605903\n",
      " Random Teilstück Error: Accuracy: 94.2%, Avg loss: 0.608221\n",
      " Random Teilstück Error: Accuracy: 95.6%, Avg loss: 0.601262\n",
      " Random Teilstück Error: Accuracy: 96.2%, Avg loss: 0.599386\n",
      " Random Teilstück Error: Accuracy: 93.4%, Avg loss: 0.613549\n",
      " Random Teilstück Error: Accuracy: 96.2%, Avg loss: 0.596087\n",
      " Random Teilstück Error: Accuracy: 96.2%, Avg loss: 0.595386\n",
      " Random Teilstück Error: Accuracy: 96.6%, Avg loss: 0.591661\n",
      " Random Teilstück Error: Accuracy: 95.1%, Avg loss: 0.608443\n",
      " Random Teilstück Error: Accuracy: 96.7%, Avg loss: 0.590812\n",
      " Random Teilstück Error: Accuracy: 96.2%, Avg loss: 0.594603\n",
      " Random Teilstück Error: Accuracy: 97.2%, Avg loss: 0.584884\n",
      " Random Teilstück Error: Accuracy: 97.2%, Avg loss: 0.584513\n",
      " Random Teilstück Error: Accuracy: 97.2%, Avg loss: 0.583418\n",
      " Random Teilstück Error: Accuracy: 97.5%, Avg loss: 0.579982\n",
      " Random Teilstück Error: Accuracy: 97.7%, Avg loss: 0.578303\n",
      " Random Teilstück Error: Accuracy: 97.5%, Avg loss: 0.581768\n",
      " Random Teilstück Error: Accuracy: 98.0%, Avg loss: 0.575942\n",
      " Random Teilstück Error: Accuracy: 97.7%, Avg loss: 0.580738\n",
      " Random Teilstück Error: Accuracy: 98.1%, Avg loss: 0.573299\n",
      " Random Teilstück Error: Accuracy: 98.2%, Avg loss: 0.571239\n",
      " Random Teilstück Error: Accuracy: 98.1%, Avg loss: 0.576429\n",
      " Random Teilstück Error: Accuracy: 98.2%, Avg loss: 0.572832\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "# training loop\n",
    "for epoch in range(50):  # loop over the dataset multiple times\n",
    "    train(train_dataloader, optimizer, criterion, model)\n",
    "    split_test(test_dataloader, optimizer, criterion, model)\n",
    "    #test_complete(test_dataloader, optimizer, criterion, model)\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4a5b2541",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "for labels, inputs in manual_dataloader:\n",
    "    inputs = inputs.to(device)\n",
    "    labels = labels.to(device)\n",
    "    logging.debug(model(inputs), labels)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
